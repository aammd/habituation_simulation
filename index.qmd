---
title: "Habituation simulation"
subtitle: "Simulations to measure the performance of a nonlinear model of habituation"
format: html
author: Andrew MacDonald
theme: minty
bibliography: references.bib
comments: 
  hypothesis: true
code-fold: false
toc: true
editor_options: 
  chunk_output_type: console
---

## Introduction

It is important to validate our statistical models before using them to reach conclusions about ecological systems. In this paper we propose a nonlinear, hierarchical statistical model to capture how Flight Initiation Distance changes with increased exposure. In this Appendix we use simulations to validate this new model, following a produced suggested by DiRenzo, Hanks and Miller [-@direnzo2023]. We use simulations to answer several distinct questions:

1.  **Which priors are appropriate?** In hierarchical nonlinear models, it is difficult to select appropriate priors by inspection. Instead we use prior predictive checks: we select values from the prior and use these to create fake datasets.
2.  **Does the model recover parameters with large datasets?** In models with many parameters, there is always the risk that parameters are difficult to estimate well. We fit a model with a large number of *Tamia* , each observed for many repeat observations, to validate that in this "idea" setting the model works well.
3.  **Does the model recover parameters in our dataset?** Our field dataset contains unequal numbers of observations for each chipmunk, and field constraints mean that sample size in particular treatment combinations is limited. We simulate observations, while keeping every aspect of our dataset the same (treatments, number of observations per individual, etc). If a model can recover true parameters here, we can be more confident about applying it to our real observations.
4.  **Can unbalanced categories lead to spurious effect size estimates?** Due to small sample effects, we do not have exactly equal representation in all of our treatment combinations. This is especially important in model 4, where we include sex and several aspects of individual personality in our model (see main text). Importantly, because of male chipmunk behaviour, males have fewer observations per individual than do females. We use simulations to confirm that effects of sex or personality on our results is not caused by the structure of the data.
5. **What is our power to measure individual variation?**  One of the main purposes of our study is to measure individual variation in habituation. We compare our model with a method popular in the literature to detect these changes. 


```{r setup, include = FALSE}
#| message: false
#| warning: false
options(tidyverse.quiet = TRUE)
knitr::opts_chunk$set(collapse = TRUE, comment = "#>", tar_simple = TRUE)
source("R/functions.R")
library(tidyverse)
library(tidybayes)
library(targets)
theme_set(theme_bw())
```


## 1. Which priors are appropriate?

### Single-individual model: math notation

We begin by evaluating the simplest case: one individual _Tamia_, observed repeatedly and habituating to a constant stimulus. We model the habituation using the following assumptions:

* _Tamia_ begin with a high FID. This value is between 0 and 1000cm, because in our experiment this was the maximum distance at which an experimenter initiated the threat treatment.
* FID declines with repeat exposures. It never goes below 0, and can only decline. While other models allow individuals to _sensitize_, we do not.
* FID does not need to decline all the way to 0, but can stop between 0 and the starting value. * FID declines such that after a certain number of exposures, the individual's FID is halfway between the starting value and the final value. The higher the number of exposures needed to get halfway to final FID, the slower the habituation.

The curve we finally ended up fitting resembles a classic TYPE II functional response curve, but inverted and scaled to correspond to our system.

This model is an adaptation of the classic Type II functional response, adapted to our study. It describes the FID of a chipmunk in three values:

* $M$, the initial flight initiation distance (proportion of 1000cm)
* $p$, the proportion of this initial FID which is lost through habituation
* $d$, the number of exposures to threat required for a chipmunk to move from $M$ to its final FID after habituation.

This is an initial version of the model, which does not contain any effect of threat treatment or any effect of personality.  


Here is the full bayesian model, including our expression for the average and all the priors.

$$
\begin{align}
\text{FID} &\sim \text{Gamma}(\alpha, \alpha/\mu) \\
\mu &= M \times 1000 \times \left(1 - \frac{p X}{d + X} \right) \\
\text{logit}(M) &= M_0 \\
\text{logit}(p) &= p_0 \\
\text{log}(d)   &= d_0 \\
M_0 &\sim N(0.5, 0.5)\\
p_0 &\sim N(-1, 0.2)\\
d_0 &\sim N(1.5, 0.5)\\
\alpha &\sim \text{Gamma}(6.25, .25)
\end{align}
$$

### Single individual model: Prior simulations

```{r}
#| fig-cap: "A simulation of 25 observations of one _Tamia_ individual. Y-axis shows flight initiation distance in cm."
one_tamia_simulation(0:25,
                     logitM = 3,
                     logitp = 5,
                     logd = .8,
                     shape =  10)[c("num_obs", "FID")] |> 
  as.data.frame() |> 
  ggplot(aes(x = num_obs, y = FID)) + 
  geom_point() + 
  labs(x = "Number of observations") + 
  theme_bw()
```

#### Different parameterizations 

:::{.callout-note}
This section is a short side note where I experiment with different ways of writing the model using Stan. It was an important first step before what comes next but it's not interesting biologically and can be skipped over.
:::

Our models need individual random effects on all parameters, and should also be flexible enough to include some individual level predictors. 
Both of these steps are made possible by placing a link function on all parameters. 
However, the creation of many nested functions can make a model both harder to read, and harder for the computer to estimate. This happens because when small numbers are multiplied together (or when large numbers are added) there is a risk that some precision will be lost in computer memory.
Models with lots of nested functions can also be slower to fit. 

I've chosen to experiment with writing this model on the log scale. 
This has a couple of advantages. First, by writing everything on the log scale, we avoid numerical problems when working with small values. 
Second, by rewriting the equation using algebra we can find ways to write the functions using Stan's built-in [composed functions](https://mc-stan.org/docs/functions-reference/composed-functions.html) which are stable and efficient.

Here is the model with link functions:

$$
\begin{align}
\text{FID} &\sim \text{Gamma}(\alpha, \alpha/\mu) \\
\ln(\mu) &= \ln(M) + 6.9 + \ln\left(1 - \frac{p \times X}{d + X} \right) \\
\text{logit}(M) &= M_0 \\
\text{logit}(p) &= p_0 \\
\text{log}(d)   &= d_0 \\
M_0 &\sim N(0.5, 0.5)\\
p_0 &\sim N(-1, 0.2)\\
d_0 &\sim N(1.5, 0.5)\\
\alpha &\sim \text{Gamma}(6.25, .25)
\end{align}
$$

in the model file `one_tamia`, we use the mathematical model as written:

```{r}
#| echo: true
#| eval: false
#| class-output: stan
#| file: stan/one_tamia.stan
```


in model `one_tamia_log`, I write the expression for the average on the log scale

```{r}
#| echo: true
#| eval: false
#| class-output: stan
#| file: stan/one_tamia_log.stan
```

in model `one_tamia_log_shape` I write the model as above, but also put the shape parameter on the log scale:

```{r}
#| echo: true
#| eval: false
#| class-output: stan
#| file: stan/one_tamia_log_shape.stan
```

Simulations on these three models show they are all equally good at recovering parameters

```{r}
tar_load(comp_param)

comp_param |> 
  filter(
    variable %in%  c("shape", 
                     "logitM",
                     "logitp", 
                     "logd")) |> 
  mutate(q50 = if_else(
    .name == "one_tamia_log_shape" & variable == "shape",
    exp(q50),
    q50)) |> 
  ggplot(aes(x = variable, y = q50)) + 
  geom_point() + 
  geom_point(aes(y = .join_data), col = "red") + 
  facet_wrap(~.name)
```

These results show that all three models give very similar inference. 
For the rest of this project I'm using the log-scale calculation as in `many_tamia_log`, because I feel it has a good balance of efficiency and readability.


### Single individual model: validation

The above was fit to a model where the observations vary but the _parameters_ all have the same values. 
However, to be sure our model works well we should fit it to a prior simulation of the dataset. 
In a prior simulation, the values of all parameters are sampled from their prior distributions. 
Then, these values are used to simulate a dataset, the model is fit to it, and we see how often the true value falls within the 95% interval of the resulting posterior.

```{r}
tar_load(prior_pred)

prior_pred |> 
  filter(variable %in% c("logitM", "logitp", "logd", "shape")) |>  
  mutate(
    q2.5 = if_else(
      .name == "one_tamia_log_shape" & variable == "shape",
      exp(q2.5),
      q2.5),
    q97.5 = if_else(
      .name == "one_tamia_log_shape" & variable == "shape",
      exp(q97.5),
      q97.5)
  ) |> 
  group_by(.name, variable) |> 
  summarize(cov_95 = sum(.join_data > q2.5 & .join_data < q97.5)/n()) |> 
  ggplot(aes(x = variable, y = cov_95)) + 
  geom_point() + 
  facet_wrap(~.name) + 
  coord_cartesian(ylim = c(0,1))
```

This shows that coverage is excellent for all parameters, with the exception of `logd`, which is recovered a little less than 80% of the time at the 95% credible interval.

### Multiple individuals: math

To extend this model to multiple individuals we need to add hyperparameters to it

In this model, individual parameters are not correlated with each other. 
The variance covariance matrix is just the identity matrix, $I$.

$$
\begin{align}
\text{FID} &\sim \text{Gamma}(\alpha, \alpha/\mu) \\
\mu &= M \times 1000 \times \left(1 - \frac{p \times X}{d + X} \right) \\
\text{logit}(M) &= M_0 + \beta_{M, i}\\
\text{logit}(p) &= p_0 + \beta_{p, i}\\
\text{log}(d)   &= d_0 + \beta_{d, i}\\
\begin{bmatrix}  \beta_{M} \  \beta_{p} \  \beta_{d} \end{bmatrix} &\sim N\left(\begin{bmatrix} 0 \ 0 \ 0 \end{bmatrix}, \Sigma\right) \\
\Sigma &= \text{diag}(\sigma_M, \sigma_p, \sigma_d) \times \text{I} \times \text{diag}(\sigma_M, \sigma_p, \sigma_d) \\
\sigma_M &\sim \text{Exponential}(4) \\
\sigma_p &\sim \text{Exponential}(2) \\
\sigma_d &\sim \text{Exponential}(2) \\
M_0 &\sim N(0.5, 0.5)\\
p_0 &\sim N(-1, 0.2)\\
d_0 &\sim N(1.5, 0.5)\\
\alpha &\sim \text{Gamma}(6.25, .25)
\end{align}
$$
We can represent this in Stan by giving each _Tamia_ individual a unique value. 

```{r}
#| echo: true
#| eval: false
#| class-output: stan
#| file: stan/many_tamia_log.stan
```

Sometimes we model correlations between parameters, and it is possible to extend the model in this way also:
[I was recently talking to Michael Betancourt, who argued pursuasively that  adding these correlations into a model is not always justified or useful! Coding them is a considerable effort and doesn't add much in our use case. We might consider dropping them.]{.aside}

$$
\begin{align}
\text{FID} &\sim \text{Gamma}(\alpha, \alpha/\mu) \\
\mu &= M \times 1000 \times \left(1 - \frac{p \times X}{d + X} \right) \\
\text{logit}(M) &= M_0 + \beta_{M, i}\\
\text{logit}(p) &= p_0 + \beta_{p, i}\\
\text{log}(d)   &= d_0 + \beta_{d, i}\\
\begin{bmatrix}  \beta_{M} \  \beta_{p} \  \beta_{d} \end{bmatrix} &\sim N\left(\begin{bmatrix} 0 \ 0 \ 0 \end{bmatrix}, \Sigma\right) \\
\Sigma &= \text{diag}(\sigma_M, \sigma_p, \sigma_d) \times \text{R} \times \text{diag}(\sigma_M, \sigma_p, \sigma_d) \\
\sigma_M &\sim \text{Exponential}(4) \\
\sigma_p &\sim \text{Exponential}(2) \\
\sigma_d &\sim \text{Exponential}(2) \\
\text{R} &\sim \text{LKJ}(2)\\
M_0 &\sim N(0.5, 0.5)\\
p_0 &\sim N(-1, 0.2)\\
d_0 &\sim N(1.5, 0.5)\\
\alpha &\sim \text{Gamma}(6.25, .25)
\end{align}
$$


```{r}
#| echo: true
#| eval: false
#| class-output: stan
#| file: stan/many_tamia_corr.stan
```

### Multiple individuals: prior predictive simulations

Simulating data from a prior is essential for defining priors that reflect knowledge about a system. Here we have chosen very general priors that cover a wide range of FID responses to threat treatment. This includes many which are biologically implausible, indicating that our prior is not too restrictive. However it excludes many cases that do not match our knowledge of the system: for example, FIDs which increase with exposure, or extremely rapid or slow habituation.

```{r}
#| label: fig-prior-pred
#| fig-cap: Simulations from the prior predictive distribution for our model of FID (Flight initiation distance). Each panel is a simulated chipmunk.

tar_load(prior_pred_data)

prior_pred_data |> 
  map_depth(2, ~.x[c("num_obs", "FID")]) |> 
  flatten() |> 
  map_df(as_tibble, .id = "rep") |>
  ggplot2::ggplot(ggplot2::aes(x = num_obs, y = FID)) + 
  ggplot2::geom_point() +  
  ggplot2::facet_wrap(~rep) + 
  labs(x = "Number of observations", y = "FID") + 
  theme_bw()
```

::: .{callout-note}
$p$ can be understood as the strength of habituation. The higher (more positive), the more of the original FID is lost over time.
:::

```{r}

map_df(1:12,
       ~n_tamia_sim_hyper(
         .max_obs = 22,
         .n_tamia = 15)[c("num_obs", "FID", "tamia_id", "mu")] |> 
         as_tibble(),
       .id = "sim") |> 
  ggplot(aes(x = num_obs, y = mu, group = tamia_id))  + 
  geom_line() + 
  facet_wrap(~sim, ncol = 4)
```


We can understand hyperparameters (i.e. $\sigma_M, \sigma_p, \sigma_d$ and $R$) by simulating and asking if they represent a plausible range of between-tamia variation. Again, we try to cover a range of plausible values while allowing for extreme cases.

### Multiple individuals: validation

I performed the same coverage tests as before, but this time on the hierarchical model for multiple individuals. 
This demonstrates that we can accurately recover parameters for this model.


```{r cov_hier}
#| layout-ncol: 2
#| fig-cap: Proportion of coverage for all parameters (left) and for all the population-level parameters (right). That is, the right-hand figure shows only the main effects, shape parameter, and hyperparameters; it excludes the individual values of M, p and d for each individual.

tar_load(cov_hier)

cov_hier |> 
  filter(variable %in% c("mu_m", "mu_p", "mu_d", "shape", "sigma_m", "sigma_d", "sigma_p")) |>  
  group_by(.name, variable) |> 
  summarize(cov_95 = sum(.join_data > q2.5 & .join_data < q97.5)/n()) |> 
  ggplot(aes(x = variable, y = cov_95)) + 
  geom_point() + 
  facet_wrap(~.name) + 
  coord_cartesian(ylim = c(0,1))

```




### Risk: math & Stan

:::{.callout-warning}
**under construction** I haven't written the math notation for these models yet!
:::

One of the advantages of a nonlinear model for FID is that we can explicitly model how all three parameters should vary in response to treatment. There are many ways to do this. Most straightforward is to allow each risk treatment to have a separate value of $M$, $p$ and $d$. (AKA a "fixed effect" model):

```{r}
#| echo: true
#| eval: false
#| class-output: stan
#| file: stan/risk_many_tamia_log.stan
```

```{r prior_pred_stan_cat}
#| fig-cap: Five simulations from the prior predictive distribution of a model of risk. This model allows the average of each parameter to vary by treatment and uses ordinal contrasts. 
tar_load(design_data)
tar_load(prior_draws_risk_many_tamia_log)
plot_prior_from_stan(prior_draws_risk_many_tamia_log,
                     design_data)
```

Another possibility is to model the effect of this factor using contrasts. Ordinal contrasts are particularly interesting, because they seem to correspond to specific hypotheses about how the perceived amount of risk should affect habituation:

```{r}
#| echo: true
#| eval: false
#| class-output: stan
#| file: stan/risk_ordinal_many_tamia_log.stan
```

```{r prior_pred_stan_ord}
#| fig-cap: Five simulations from the prior predictive distribution of a model of risk. This model allows the average of each parameter to vary by treatment and uses ordinal contrasts. 
tar_load(design_data)
tar_load(prior_draws_risk_ordinal_many_tamia_log)
plot_prior_from_stan(prior_draws_risk_ordinal_many_tamia_log,
                     design_data)
```


## Design data

So far, the simulations have used an idealized dataset where all individuals are observed a large number of times. 
In the following section I assess the model's performance on a data simulation based on the real dataset. 
In reality, different individuals are not observed the same number of times.
Observations, in this dataset, have a double impact on the model: they are simultaneously the independent variable and also represent greater power. 
Therefore its important to find out if a realistic distribution of observations per individual actually results in less power or worse performace.

```{r}
#| layout-ncol: 2
#| fig-cap: "Simulations from the prior distribution based on the true dataset."
tar_load(design_sim)

design_simulation <- design_sim[c("num_obs", "tamia_id", "FID", "mu")] |> as.data.frame()
  

design_simulation |> 
  ggplot(aes(x = num_obs, y = mu, group = tamia_id)) + geom_line()

design_simulation |> 
  ggplot(aes(x = num_obs, y = FID, group = tamia_id)) + geom_point()

```

Fit to many of these and look at posterior coverage

:::{.callout-warning}
## under construction
I performed simulations using brms and showed that the design of Catherine's experiment allows parameters to be recovered very well. I have yet to redo this with the Stan models.
It might change a bit as a result.
:::


## Model 4: many predictors

:::{.callout-warning}
I'm not sure to what extent this is still required or interesting! The models directly above (i.e. using Risk as a factor) are already a good step towards this.
:::

In model 4, we attempt to explain individual differences in the parameters $M$, $p$ and $d$ by adding individual-level predictor variables to our model. These are sex, number of previous captures, Docility, and Explorativeness. However, such effects might be difficult to detect in our experiment, as this represents more parameters to estimate. This is especially important for sex: male chipmunks are less likely to remain in the study area, and therefore are less likely to be observed frequently. This results in less opportunity to observe their habituation.  It might also result in a biased estimate of differences between the sexes.

To investigate the possibility of spurious effects of individual-level predictors, we simulated data using Model 1 above. This model does not have any effects of the four predictors above. We then fit a model which contains all the predictor variables. 

$$
\begin{align}
\text{FID} &\sim \text{Gamma}(\alpha, \alpha/\mu) \\
\mu &= M \times 1000 \times \left(1 - \frac{p \times X}{d + X} \right) \\
\text{logit}(M) &= M_0 + \beta_{M, i} + \beta_{\text{Risk}[i]} + \beta_c \text{Captures}\\
\text{logit}(p) &= p_0 + \beta_{p, i} + \beta_{\text{Risk}[i]} + \beta_{\text{sex}[i]} + \beta_d \text{Docility} + \beta_x \text{Explorative}\\
\text{log}(d)   &= d_0 + \beta_{d, i} + \beta_{\text{Risk}[i]}\\
\begin{bmatrix}  \beta_{M} \  \beta_{p} \  \beta_{d} \end{bmatrix} &\sim N\left(\begin{bmatrix} 0 \ 0 \ 0 \end{bmatrix}, \Sigma\right) \\
\Sigma &= \text{diag}(\sigma_M, \sigma_p, \sigma_d) \times \text{R} \times \text{diag}(\sigma_M, \sigma_p, \sigma_d) \\
\sigma_M &\sim \text{Exponential}(4) \\
\sigma_p &\sim \text{Exponential}(2) \\
\sigma_d &\sim \text{Exponential}(2) \\
\text{R} &\sim \text{LKJ}(2) \\
M_0 &\sim N(0.5, 0.5)\\
p_0 &\sim N(-1, 0.2)\\
d_0 &\sim N(1.5, 0.5)\\
\alpha &\sim \text{Gamma}(6.25, .25)
\end{align}
$$

The simulation produces chipmunk which vary individually, and which have the same number of observations per individual as the real dataset. We then analyzed this model with Model 4. Since there are no effects of individual-level predictors present in the model (other than Risk), We should find the posterior distributions to be close to 0 for $\beta_c$, $\beta_x$, $\beta_d$ and $\beta_{sex}$

```{r}
# targets::tar_load(model4_posterior_plot)
# model4_posterior_plot

```


```{r}
# tar_load(nonzero_model4_table)
# knitr::kable(nonzero_model4_table)
```

In general, the model finds posteriors that are close to 0 for all coefficients. However, there is a surprising bias towards spurious sex-differences in the value of $p$. This is possibly due to the lower sample size of male chipmunks


# Transformations

A common way to work with this kind of data is to transform it, and then to use a mixed-model approach. We want to test the power of these models to discover individual variation, and compare it to our proposed model for FID. 

Which model is better at detecting individual variation in habituation? Here I'm defining habituation in a specific way -- that while individuals might _start_ at different FID, their response to successive exposures to a stimulus is different. That is, there is individual variation in the "slope". I consider that a model shows evidence for individual variation in habituation when an information criterion (LOO-IC in this case) prefers a model with individual slopes over one where all models have the same slope.

In math, the code for a transformed model looks like this. the function $g()$ could be anything; in our test we use $g(x) = \sqrt{x}$ and $g(x) = \ln(x)$

$$
\begin{align}
g(\text{FID}_i) &\sim \text{Normal}(\mu_i, \sigma)\\
\mu_i &= \beta_{0, \text{indiv}[i]} + \beta_1\times(\text{exposure})
\end{align}
$$

Which is compared to a model where slopes **vary** for each individual

$$
\begin{align}
g(\text{FID}_i) &\sim \text{Normal}(\mu_i, \sigma)\\
\mu_i &= \beta_{0, \text{indiv}[i]} + \beta_{1, \text{indiv}[i]}\times(\text{exposure})
\end{align}
$$


## Stan code

#### Log-transformed response

A model with constant slope
```{r}
#| class-output: stan
#| comment: ''
tar_load(non_var_log)
non_var_log
```

A model with individual slopes
```{r}
#| class-output: stan
#| comment: ''
tar_load(oui_var_log)
oui_var_log
```

#### Square root transformed response

a model with constant slope
```{r}
#| class-output: stan
#| comment: ''
tar_load(non_var_sqr)
non_var_sqr$print()
```

A model with individual slopes
```{r}
#| class-output: stan
#| comment: ''
tar_load(oui_var_sqr)
oui_var_sqr
```

#### Our proposed nonlinear model

a model with constant slope
```{r}
#| class-output: stan
#| comment: ''
tar_load(non_var_nonlin)
non_var_nonlin
```

A model with individual slopes
```{r}
#| class-output: stan
#| comment: ''
tar_load(oui_var_nonlin)
oui_var_nonlin
```


## Results 

```{r}
tar_load(fig_panel)
fig_panel
```


On the left is the average FID, on the right is observations around this average (note the slightly different scale). note that this variation is much lower than the variation we found in our dataset.  There is also individual variation here, a bit greater than what was found in the real dataset.

I simulated many datasets that looked like this one. For each, I fit a pair of models. One model has individual differences in habituation (i.e. close to the truth); the other model has constant habituation for all individuals. For each dataset, I compared both models using LOO-IC (basically a bayesian AIC). The following figure shows the model comparison output. More negative numbers mean the model WITH variation is winning (the numbers measure how much lower the LOO-IC score is for the constant-habituation model)

These results show the increased power of our approach in comparison to two others: log transformed FID and square-root transformed FID.  I think it is interesting that it seems a square-root model isn't able to detect individual difference, even at small sample size.

Its not that surprising that the nonlinear model fits well, since that is the data-generating model. What's interesting to me is that the square root, and even the log - transformed model have quite weak results.



